# RAG & Semantic Search Implementation

Complete guide to Listopia's Retrieval-Augmented Generation (RAG) and semantic search system. This document covers how embeddings, vector search, and RAG context work together to enhance the chat system.

---

## Table of Contents

1. [Overview](#overview)
2. [Core Concepts](#core-concepts)
3. [Architecture](#architecture)
4. [Database Setup](#database-setup)
5. [Embedding Generation](#embedding-generation)
6. [Hybrid Search](#hybrid-search)
7. [RAG Integration](#rag-integration)
8. [Search vs. Navigation Decision Tree](#search-vs-navigation-decision-tree)
9. [API Endpoints](#api-endpoints)
10. [Internal Services](#internal-services)
11. [Integration with Chat](#integration-with-chat)
12. [Performance & Optimization](#performance--optimization)
13. [Troubleshooting](#troubleshooting)
14. [Future Enhancements](#future-enhancements)

---

## Overview

Listopia's semantic search system combines three complementary approaches:

### 1. **Vector Search (Semantic)**
Uses OpenAI embeddings to find semantically similar content based on meaning, not just keywords.

```
User Query: "I need a project timeline"
  ↓
Generate embedding for query
  ↓
Search for similar embeddings in database
  ↓
Results: Lists about schedules, milestones, planning
```

### 2. **Full-Text Search (Keyword)**
Uses PostgreSQL's full-text search for exact keyword matching and keyword combinations.

```
User Query: "budget planning"
  ↓
Search for documents containing "budget" AND "planning"
  ↓
Results: Lists with exact words in title/description
```

### 3. **Hybrid Search**
Combines vector + full-text scores for best of both worlds.

```
User Query: "how to implement authentication"
  ↓
Vector search: Finds docs about security, login, auth
  ↓
Keyword search: Finds docs with exact words "implement", "authentication"
  ↓
Combine scores: Return ranked results
```

### 4. **RAG (Retrieval-Augmented Generation)**
Uses search results to provide context to the LLM for better responses.

```
User Question in Chat: "What's our security strategy?"
  ↓
Search for related documents (vector + keyword)
  ↓
Build context from top 5 results
  ↓
Send context + question to LLM
  ↓
LLM generates better answer with real data
```

---

## Core Concepts

### Embeddings

An embedding is a numerical representation of text meaning. Generated by OpenAI's `text-embedding-3-small` model.

```
Text: "Project planning for Q4 2025"
  ↓
Embedding: [0.1234, -0.5678, 0.9012, ...] (1536 dimensions)
  ↓
Stored in database for similarity search
```

**Key Properties:**
- **Dimensionality**: 1536 dimensions
- **Model**: OpenAI `text-embedding-3-small`
- **Cost**: ~$0.02 per 1M tokens (very inexpensive)
- **Speed**: ~100ms per request
- **Updating**: Generated on record creation/update

### Vector Similarity Search

Finds documents with embeddings closest to query embedding (cosine similarity).

```
Query: "deadline for marketing project"
Query Embedding: [0.2, -0.3, 0.5, ...]
  ↓
Compare to each record's embedding using cosine distance
  ↓
Return top matches (closest vectors)
  ↓
Results: Lists about timelines, projects, marketing
```

### Full-Text Search

PostgreSQL searches for word combinations in documents.

```
Query: "marketing deadline"
  ↓
Convert to PostgreSQL search document: marketing & deadline
  ↓
Search GIN index for matching documents
  ↓
Return matches ranked by relevance
```

### Relevance Scoring

Combines both search methods into single score:

```
Relevance = (Vector Score × 0.6) + (Keyword Score × 0.4)
```

Adjust weights based on your priorities:
- More weight on vector search = better semantic matching
- More weight on keyword search = better exact matching

---

## Architecture

### System Components

```
┌─────────────────────────────────────────────────────┐
│ User Input (Search, Chat Question, Navigation)      │
└──────────────────┬──────────────────────────────────┘
                   ↓
        ┌──────────────────────┐
        │   SearchService      │
        │  (Hybrid Search)     │
        └──────────┬───────────┘
                   ↓
        ┌──────────────────────┐
        │  Vector Search       │ Uses embeddings
        │  (Semantic)          │
        └──────────┬───────────┘
                   │
        ┌──────────────────────┐
        │  Keyword Search      │ Full-text search
        │  (PostgreSQL)        │
        └──────────┬───────────┘
                   ↓
        ┌──────────────────────┐
        │   RAG Service        │
        │  (Context Building)  │
        └──────────┬───────────┘
                   ↓
        ┌──────────────────────┐
        │   LLM                │
        │  (With Context)      │
        └──────────────────────┘

Background:
┌──────────────────────────────────┐
│  Record Saved (List, Item, etc.) │
└──────────────┬───────────────────┘
               ↓
   ┌─────────────────────────┐
   │ EmbeddingGenerationJob  │
   │   (Async, Solid Queue)  │
   └────────────┬────────────┘
                ↓
   ┌─────────────────────────┐
   │EmbeddingGenerationService│
   │  (Call OpenAI API)      │
   └────────────┬────────────┘
                ↓
   ┌─────────────────────────┐
   │  Save Embedding         │
   │  + Update Index         │
   └─────────────────────────┘
```

### Models & Services

| Component | Purpose | File |
|-----------|---------|------|
| `SearchableEmbeddable` | Concern for embedding lifecycle | `app/models/concerns/searchable_embeddable.rb` |
| `EmbeddingGenerationService` | Calls OpenAI API, saves embeddings | `app/services/embedding_generation_service.rb` |
| `EmbeddingGenerationJob` | Async job via Solid Queue | `app/jobs/embedding_generation_job.rb` |
| `SearchService` | Hybrid search (vector + keyword) | `app/services/search_service.rb` |
| `RagService` | Builds context from search results | `app/services/rag_service.rb` |
| `SearchController` | Web interface and JSON API | `app/controllers/search_controller.rb` |

---

## Database Setup

### Required Extensions

PostgreSQL must have pgvector extension enabled:

```sql
CREATE EXTENSION IF NOT EXISTS pgvector;
```

### Migrations

Run these migrations to set up search infrastructure:

#### 1. Add Embedding Vectors

```ruby
# db/migrate/20251208050100_add_embedding_vectors.rb
class AddEmbeddingVectors < ActiveRecord::Migration[8.0]
  def change
    # Add vector columns for Lists, ListItems, Comments, Tags
    add_column :lists, :embedding, :vector, limit: 1536
    add_column :list_items, :embedding, :vector, limit: 1536
    add_column :comments, :embedding, :vector, limit: 1536

    # Track embedding freshness
    add_column :lists, :embedding_generated_at, :datetime
    add_column :lists, :requires_embedding_update, :boolean, default: false

    # Create IVFFLAT indexes for fast similarity search
    add_index :lists, :embedding, using: :ivfflat, opclass: :vector_cosine_ops
    add_index :list_items, :embedding, using: :ivfflat, opclass: :vector_cosine_ops
    add_index :comments, :embedding, using: :ivfflat, opclass: :vector_cosine_ops
  end
end
```

#### 2. Add Full-Text Search Support

```ruby
# db/migrate/20251208050101_add_fulltext_search_support.rb
class AddFulltextSearchSupport < ActiveRecord::Migration[8.0]
  def change
    # Add TSVECTOR columns for full-text search
    add_column :lists, :search_document, :tsvector
    add_column :list_items, :search_document, :tsvector
    add_column :comments, :search_document, :tsvector

    # Create GIN indexes for fast keyword search
    add_index :lists, :search_document, using: :gin
    add_index :list_items, :search_document, using: :gin
    add_index :comments, :search_document, using: :gin
  end
end
```

### Run Migrations

```bash
rails db:migrate

# Verify extensions
rails db:execute:sql "SELECT extname FROM pg_extension WHERE extname = 'pgvector'"
# Should return: pgvector
```

---

## Embedding Generation

### Automatic Generation

Embeddings are generated automatically when records are created or updated:

```ruby
# When a list is created
list = List.create!(title: "Q4 Planning", description: "...")
# → EmbeddingGenerationJob scheduled automatically

# When a list is updated
list.update!(title: "Q4 Planning 2025")
# → Embedding marked stale, job scheduled
```

### Searchable Models

These models automatically generate embeddings:

```ruby
class List < ApplicationRecord
  include SearchableEmbeddable
  # Automatically embeds: title + description
end

class ListItem < ApplicationRecord
  include SearchableEmbeddable
  # Automatically embeds: title + description
end

class Comment < ApplicationRecord
  include SearchableEmbeddable
  # Automatically embeds: content
end

class ActsAsTaggableOn::Tag
  include SearchableEmbeddable
  # Automatically embeds: tag name
end
```

### Manual Generation (if needed)

```ruby
# Generate embedding for a single record
EmbeddingGenerationService.new(list).call

# Generate embeddings for all records needing updates
List.needs_embedding.find_each do |list|
  EmbeddingGenerationJob.perform_later(list.id, 'List')
end

# Monitor progress
puts "Pending: #{List.needs_embedding.count}"
puts "Stale: #{List.stale_embeddings.count}"
```

### Monitoring Embedding Generation

```ruby
# Check embedding status
list = List.first
puts list.embedding_generated_at
puts list.requires_embedding_update?
puts list.embedding.present?

# Check which models have embeddings
List.where("embedding IS NOT NULL").count
ListItem.where("embedding IS NOT NULL").count
```

---

## Hybrid Search

### SearchService API

The `SearchService` provides hybrid search combining vector + keyword approaches:

```ruby
# Basic search
result = SearchService.call(
  query: "implement authentication",
  user: current_user
)

if result.success?
  results = result.data  # Array of records
else
  errors = result.errors
end
```

### Advanced Options

```ruby
# Search specific models only
result = SearchService.call(
  query: "budget planning",
  user: current_user,
  models: [List, ListItem],  # Only search lists and items
  limit: 10,                   # Max 10 results
  use_vector: true            # Enable vector search (default)
)

# Fallback to keyword-only search
result = SearchService.call(
  query: "authentication",
  user: current_user,
  use_vector: false  # Vector search disabled
)
```

### How Hybrid Search Works

```
1. Prepare Query
   - Get embedding from OpenAI API
   - Prepare keyword search document

2. Vector Search (50% weight)
   - Search for embedding similarity
   - Calculate cosine distance scores
   - Return top matches

3. Keyword Search (50% weight)
   - Search for text matches
   - Calculate PostgreSQL relevance
   - Return top matches

4. Combine Results
   - Merge both result sets
   - Weight: (Vector × 0.5) + (Keyword × 0.5)
   - Sort by combined score
   - Limit to requested count
   - Apply authorization filters
```

### Authorization Filtering

Results automatically respect user's organization:

```ruby
# SearchService automatically:
# 1. Queries only user's organizations
# 2. Filters by visibility (public/private lists)
# 3. Checks if user can access parent resources
# 4. Uses policy_scope for additional filters

result = SearchService.call(query: "finance", user: user_in_org_a)
# Only returns results from org_a, not org_b
```

---

## RAG Integration

### RagService

Builds context from search results to enhance LLM responses:

```ruby
# In chat, when user asks a question
question = "What's our security strategy?"

# 1. Search for relevant context
rag_result = RagService.call(
  query: question,
  user: current_user,
  limit: 5  # Get top 5 results
)

# 2. Get context and sources
context = rag_result.data[:context]  # Formatted context
sources = rag_result.data[:sources]  # Source citations

# 3. Send to LLM with context
response = RubyLLM.chat(
  messages: [
    { role: "system", content: context },  # RAG context
    { role: "user", content: question }
  ]
)

# 4. Return with source attribution
Message.create_assistant(
  chat: chat,
  content: response,
  metadata: {
    rag_context: context,
    rag_sources: sources
  }
)
```

### Context Building

```ruby
# RagService generates:

# 1. System Prompt with Context
"""
You are a helpful assistant for Listopia.
You have access to the following relevant information:

[Source 1] List: "Q4 Planning"
Description: Details about quarterly planning...

[Source 2] Item: "Security audit deadline"
Description: Milestone for security review...

Use this information to answer user questions accurately.
"""

# 2. Source Attribution
[
  {
    type: "List",
    title: "Q4 Planning",
    url: "/lists/uuid-1",
    relevance: 0.95
  },
  {
    type: "ListItem",
    title: "Security audit",
    url: "/lists/uuid-parent/items/uuid-2",
    relevance: 0.87
  }
]
```

### RAG Configuration

In chat context, control RAG behavior:

```ruby
# Enable/disable RAG for a chat
chat.metadata = {
  rag_enabled: true,  # Use RAG context
  rag_limit: 5,       # Top 5 results
  rag_weighting: :balanced  # balanced|semantic|keyword
}

# Different weighting strategies
:semantic  # Prefer vector search (0.7 × vector + 0.3 × keyword)
:balanced  # Equal weight (0.5 × vector + 0.5 × keyword)
:keyword   # Prefer keyword (0.3 × vector + 0.7 × keyword)
```

---

## Search vs. Navigation Decision Tree

When user asks something, decide whether to use search, navigation, or LLM:

```
User: "Show me budget lists"
  ├─ Navigation Match? ("Show me" + known page)
  │  └─ YES → Navigate to /lists?query=budget
  │
  ├─ Command Match? (/search, /help, etc.)
  │  └─ YES → Execute command
  │
  ├─ Should Search? (Looking for information)
  │  ├─ YES → SearchService.call(query: "budget")
  │  │  └─ Display results with links
  │  │
  │  └─ NO → Use LLM
  │     ├─ With RAG context? (Chat enabled RAG)
  │     │  └─ YES → RagService.call + send to LLM
  │     │
  │     └─ Without RAG → Direct LLM response

```

### Decision Criteria

**Use Navigation:**
- User wants to go TO a page
- Pattern: "Show me", "Go to", "Take me to"
- Example: "Show me all users" → `/admin/users`

**Use Search:**
- User wants to FIND something
- Pattern: "Find", "Search for", "What lists..."
- Example: "Search for budget" → SearchService results

**Use LLM (with RAG):**
- User wants an ANSWER or ANALYSIS
- Pattern: "How", "What's", "Explain", "Help me"
- Example: "How should we approach budgeting?" → LLM + RAG context

**Use LLM (without RAG):**
- User wants GENERAL ADVICE
- Pattern: "How do I", "What's the best way"
- Example: "How do I use Listopia?" → LLM only

---

## API Endpoints

### Search Web Interface

```
GET /search?q=query&limit=20
```

**Parameters:**
- `q` (string, required): Search query
- `limit` (integer, optional): Max results (default: 20)

**Response:** HTML page with formatted results

**Example:**
```
GET /search?q=implement+authentication
GET /search?q=meeting&limit=5
```

### Search JSON API

```
GET /search.json?q=query&limit=20
```

**Parameters:**
- `q` (string, required): Search query
- `limit` (integer, optional): Max results (default: 20)

**Response:**
```json
{
  "query": "test",
  "results": [
    {
      "id": "uuid-123",
      "type": "List",
      "title": "Test Planning",
      "description": "Plan the test suite",
      "url": "/lists/uuid-123",
      "created_at": "2025-01-15T10:00:00Z",
      "updated_at": "2025-01-15T10:00:00Z"
    }
  ],
  "count": 1
}
```

**Status Codes:**
- `200 OK` - Search successful
- `401 Unauthorized` - User not authenticated
- `400 Bad Request` - Missing query parameter

---

## Internal Services

### SearchService

Hybrid search across multiple models:

```ruby
result = SearchService.call(
  query: "implement auth",
  user: current_user,
  models: [List, ListItem, Comment],  # Optional
  limit: 20,                           # Optional
  use_vector: true                     # Optional
)

if result.success?
  results = result.data  # Array of records
else
  errors = result.errors
end
```

**Features:**
- Hybrid approach combining vector + full-text search
- Authorization enforcement (org boundaries)
- Fallback to keyword-only if vector unavailable
- Ranking by relevance, recency, model type

### EmbeddingGenerationService

Generates and stores embeddings:

```ruby
service = EmbeddingGenerationService.new(list)
result = service.call

if result.success?
  puts "Embedding generated and stored"
else
  puts "Error: #{result.errors}"
end
```

**Process:**
1. Prepare content for embedding
2. Call OpenAI embedding API
3. Save embedding to database
4. Update `embedding_generated_at`
5. Mark `requires_embedding_update` as false

### RagService

Builds LLM context from search results:

```ruby
result = RagService.call(
  query: user_question,
  user: current_user,
  limit: 5,  # Top 5 results
  weighting: :balanced  # semantic|balanced|keyword
)

if result.success?
  context = result.data[:context]  # System prompt with context
  sources = result.data[:sources]  # Citation information
end
```

---

## Integration with Chat

### Adding RAG to Chat Completions

```ruby
# app/services/chat_completion_service.rb

def call
  # 1. Check if RAG is enabled for this chat
  if @chat.metadata[:rag_enabled]
    # 2. Get RAG context from user message
    rag_result = RagService.call(
      query: @current_message.content,
      user: @user,
      limit: @chat.metadata[:rag_limit] || 5
    )

    if rag_result.success?
      context = rag_result.data[:context]
      sources = rag_result.data[:sources]
    end
  end

  # 3. Call LLM with context
  llm_response = call_llm_with_context(context)

  # 4. Store with source attribution
  Message.create_assistant(
    chat: @chat,
    content: llm_response,
    metadata: {
      rag_enabled: @chat.metadata[:rag_enabled],
      rag_sources: sources
    }
  )
end
```

### Rendering RAG Sources in Chat

```erb
<!-- app/views/message_templates/_rag_sources.html.erb -->
<div class="rag-sources">
  <p class="text-xs font-semibold text-gray-600">Sources:</p>
  <ol class="space-y-1">
    <% sources.each_with_index do |source, idx| %>
      <li class="text-sm">
        [<%= idx + 1 %>]
        <a href="<%= source[:url] %>" class="text-blue-600 hover:underline">
          <%= source[:type] %>: <%= source[:title] %>
        </a>
        <span class="text-gray-500">(<%= "%.0f%%" % (source[:relevance] * 100) %>)</span>
      </li>
    <% end %>
  </ol>
</div>
```

### Toggle RAG in Chat Settings

```ruby
# app/controllers/chats_controller.rb

def update_rag_setting
  @chat.metadata[:rag_enabled] = params[:enabled]
  @chat.save!

  respond_to do |format|
    format.turbo_stream
  end
end
```

---

## Performance & Optimization

### Embedding Generation Performance

**Cost:** ~$0.02 per 1M tokens
- Average list: 100 tokens = $0.000002
- 10,000 lists: ~$0.02

**Speed:** Generated asynchronously via Solid Queue
- Doesn't block user interaction
- Processes in background

**Optimization:**
```ruby
# Batch embed updates
List.stale_embeddings.find_in_batches do |batch|
  batch.each { |list| EmbeddingGenerationJob.perform_later(list.id) }
end
```

### Search Performance

**Vector Search:**
- IVFFLAT index: ~10-50ms for similarity
- Scales to millions of records
- Configurable trade-off: speed vs. accuracy

**Keyword Search:**
- GIN index: ~5-20ms for keyword match
- Scales well with standard PostgreSQL optimization

**Hybrid Search:**
- Total: ~50-100ms for combined search
- Network latency dominant factor

**Optimization:**
```ruby
# Use pagination for large result sets
result = SearchService.call(
  query: "budget",
  user: user,
  limit: 20  # Don't return thousands of results
)

# Cache frequent searches
Rails.cache.fetch("search:#{query}", expires_in: 15.minutes) do
  SearchService.call(query: query, user: user).data
end
```

### Database Optimization

**Index Maintenance:**
```sql
-- Monitor index sizes
SELECT indexname, pg_size_pretty(pg_relation_size(indexrelid)) AS size
FROM pg_stat_user_indexes
WHERE schemaname = 'public';

-- Analyze tables for query planning
ANALYZE lists;
ANALYZE list_items;
```

**Vector Index Configuration:**
```ruby
# IVFFLAT settings in migration
execute "CREATE INDEX idx_lists_embedding ON lists USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100)"

# Adjust 'lists' parameter (100-1000) based on:
# - Database size: bigger = more lists
# - Speed requirement: higher = faster but more memory
```

---

## Troubleshooting

### Embeddings Not Generated

**Problem:** Records created but embedding is NULL

**Solutions:**
```ruby
# 1. Check if job is queued
require 'solid_queue'
SolidQueue::Job.all.count

# 2. Check job failures
SolidQueue::FailedExecution.all

# 3. Manually generate
EmbeddingGenerationService.new(list).call

# 4. Check OpenAI API key
ENV['OPENAI_API_KEY'].present?
```

### Search Returns No Results

**Problem:** Query doesn't find expected results

**Solutions:**
```ruby
# 1. Check if records have embeddings
List.where("embedding IS NULL").count

# 2. Test vector search directly
vector = EmbeddingGenerationService.get_embedding("query")
# Should return 1536-dim vector

# 3. Check keyword search
List.where("search_document @@ to_tsquery('budget')")

# 4. Lower similarity threshold
# (In SearchService, adjust cosine distance requirement)
```

### Embedding Generation Errors

**Problem:** Job fails with OpenAI API error

**Solutions:**
```ruby
# Check OpenAI API
RubyLLM.models  # Should list available models

# Test embedding endpoint
RubyLLM.embedding(input: "test", model: "text-embedding-3-small")

# Check rate limits
# (OpenAI has rate limits, stagger job processing)

# Increase retry limit
EmbeddingGenerationJob.set(wait: 30.seconds, attempts: 5)
```

### Slow Search Queries

**Problem:** Search takes >1 second

**Solutions:**
```ruby
# 1. Check index usage
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM lists
WHERE embedding <-> 'vector' LIMIT 10;

# 2. Rebuild indexes if fragmented
REINDEX INDEX idx_lists_embedding;

# 3. Increase work_mem for faster operations
SET work_mem = '256MB';

# 4. Use limit to reduce results
SearchService.call(query: q, limit: 10)  # Not 1000
```

---

## Future Enhancements

### 1. Custom Embeddings per Organization

Allow organizations to use their own embedding models:

```ruby
# In organization settings
organization.metadata = {
  embedding_model: "custom-model-id",
  embedding_api_key: "org-specific-key"
}
```

### 2. Semantic Caching

Cache embeddings and search results:

```ruby
# Check cache before embedding
cached = Rails.cache.read("embedding:#{content}")
if cached
  embedding = cached
else
  embedding = EmbeddingGenerationService.get_embedding(content)
  Rails.cache.write("embedding:#{content}", embedding, expires_in: 30.days)
end
```

### 3. Feedback Loop

Use search feedback to improve relevance:

```ruby
# Track when user finds what they searched for
SearchFeedback.create(
  query: "budget",
  result_id: list.id,
  helpful: true
)

# Use feedback to adjust weighting
# Results users find helpful → increase relevance weight
```

### 4. Advanced RAG Features

- **Multi-source RAG:** Combine with external data sources
- **Fact verification:** Check LLM responses against sources
- **Citation tracking:** Automatic source citations in responses
- **Confidence scoring:** Show confidence in LLM responses

### 5. Semantic Clustering

Group similar documents:

```ruby
# Cluster lists by semantic similarity
clusters = SemanticClusteringService.call(user: user)
# Returns: [[similar_lists], [similar_lists], ...]
```

### 6. Embedding Model Upgrades

Use newer, more capable models:

```ruby
# Upgrade to text-embedding-3-large (3072 dims)
EmbeddingGenerationService::MODEL = "text-embedding-3-large"
# Better accuracy but higher cost (~3x)
```

---

## Summary

Listopia's RAG & semantic search system provides:

✅ **Intelligent Search** - Find information by meaning, not just keywords
✅ **Context-Aware LLM** - RAG provides real data to the chat system
✅ **Organization Privacy** - All results scoped to user's organization
✅ **Automatic Updates** - Embeddings generated on record changes
✅ **Scalable Design** - Handles millions of records efficiently
✅ **Fallback Support** - Works without embeddings (keyword-only search)
✅ **Monitoring** - Track embedding generation and search performance

**Next Steps:**
1. Run migrations to enable pgvector
2. Test embedding generation with a sample record
3. Try SearchService with your data
4. Integrate RAG context into chat responses
5. Monitor performance and adjust as needed

For chat integration details, see [CHAT_FEATURES.md](CHAT_FEATURES.md)
